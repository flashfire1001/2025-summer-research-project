Perfect! Here's the **complete design summary block** for your **U-Net + Flow Matching + Classifier-Free Guidance (CFM)** model trained on **CIFAR-10**, including architecture and **training hyperparameters**.

------

## üß± U-Net + FM + CFM on CIFAR-10 ‚Äî Design Summary

### üñºÔ∏è **Model Structure**

| Component           | Description                                                  |
| ------------------- | ------------------------------------------------------------ |
| **Input**           | Image `x ‚àà ‚Ñù^{3√ó32√ó32}`, time `t ‚àà [0, 1]`, label `y ‚àà {0, ..., 9}` or `None` |
| **Time Embedding**  | Fourier features ‚Üí MLP ‚Üí `t_emb ‚àà ‚Ñù^{t_dim}`                 |
| **Class Embedding** | `nn.Embedding(num_classes, y_dim)` ‚Üí MLP ‚Üí `y_emb ‚àà ‚Ñù^{y_dim}` |
| **Conditioning**    | Inject `t_emb` and `y_emb` into every residual block         |

------

### üîß **U-Net Architecture (Residual)**

```
Input: 3√ó32√ó32 image

‚Üì Conv2d(3 ‚Üí 64)

‚Üì Encoder 1 (32√ó32 ‚Üí 16√ó16)
   ResBlock(64‚Üí64, t_emb, y_emb)
   ResBlock(64‚Üí128, t_emb, y_emb)
   Downsample (stride=2)

‚Üì Encoder 2 (16√ó16 ‚Üí 8√ó8)
   ResBlock(128‚Üí128)
   ResBlock(128‚Üí256, t_emb, y_emb)
   Downsample

‚Üì Encoder 3 (8√ó8 ‚Üí 4√ó4)
   ResBlock(256‚Üí256)
   ResBlock(256‚Üí512, t_emb, y_emb)
   Downsample

‚Üì Bottleneck (4√ó4)
   ResBlock(512‚Üí512, t_emb, y_emb)
   ResBlock(512‚Üí512, t_emb, y_emb)

‚Üë Decoder 1 (4√ó4 ‚Üí 8√ó8)
   Upsample
   Concat skip
   ResBlock(1024‚Üí512)
   ResBlock(512‚Üí256, t_emb, y_emb)

‚Üë Decoder 2 (8√ó8 ‚Üí 16√ó16)
   Upsample
   Concat skip
   ResBlock(512‚Üí256)
   ResBlock(256‚Üí128, t_emb, y_emb)

‚Üë Decoder 3 (16√ó16 ‚Üí 32√ó32)
   Upsample
   Concat skip
   ResBlock(256‚Üí128)
   ResBlock(128‚Üí64, t_emb, y_emb)

‚Üë Final Conv2d(64 ‚Üí 3)
Output: velocity field v(x, t, y) ‚àà ‚Ñù^{3√ó32√ó32}
```

>   Residual block = GroupNorm + SiLU + Conv + time/label conditioning + residual connection

------

### ‚öôÔ∏è **Training Hyperparameters**

| Hyperparameter    | Recommended Value                | Notes                                      |
| ----------------- | -------------------------------- | ------------------------------------------ |
| **Batch size**    | `64` (or `128` if memory allows) | Tradeoff between stability and speed       |
| **Learning rate** | `1e-4` (use Adam)                | Try `5e-5` if training unstable            |
| **Optimizer**     | `Adam(Œ≤1=0.9, Œ≤2=0.999)`         | Standard setup for generative training     |
| **Weight decay**  | `0.0` or `1e-5`                  | Optional; small value helps regularization |
| **Gradient clip** | `1.0`                            | Helps prevent spikes in training           |
| **LR scheduler**  | Fixed or Cosine Annealing        | Cosine can help late-stage stability       |
| **EMA**           | `0.999` decay                    | Optional but helps final sampling quality  |

------

### ‚è±Ô∏è **Training Schedule**

| Setting             | Value                       | Notes                            |
| ------------------- | --------------------------- | -------------------------------- |
| **Max Epochs**      | `500‚Äì1000`                  | For full convergence on CIFAR-10 |
| **Steps per Epoch** | `50,000 / batch_size ‚âà 782` | Batch size = 64                  |
| **Early Stopping**  | `patience = 20 epochs`      | Monitor val loss or FID          |
| **Min Œî (delta)**   | `1e-4`                      | For stopping criterion           |

------

### üß™ **Classifier-Free Guidance (CFM)**

| Component                | Setting                                                      | Notes                              |
| ------------------------ | ------------------------------------------------------------ | ---------------------------------- |
| **Dropout Rate**         | `10%‚Äì20%` of label embeddings                                | Drop `y` at random during training |
| **Guidance Scale**       | `2.0 ‚Äì 6.0`                                                  | Tune at inference time             |
| **Conditioned Sampling** | v(x, t, y) = v(x, t, null) + scale √ó (v(x, t, y) - v(x, t, null)) | Standard CFG formula               |

------

### üìå Summary Recommendations

-   ‚úÖ Use **residual blocks** throughout with time and label embedding
-   ‚úÖ Normalize input images to `[-1, 1]`
-   ‚úÖ Use **U-Net with 3 downsampling stages** (sufficient for 32√ó32)
-   ‚úÖ Train for 500k‚Äì1M iterations with **EMA**, **Adam**, and **CFM dropout**
-   ‚úÖ Use fixed LR or cosine decay, and monitor val loss or FID for early stopping

------

Let me know if you'd like a full PyTorch training loop or sampling script based on this setup!