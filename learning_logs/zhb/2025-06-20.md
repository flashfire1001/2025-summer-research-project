1. å®ç°äº†æœ€åŸºæœ¬çš„MLPç¥ç»ç½‘ç»œè®­ç»ƒåˆ†ç±»MNISTæ•°æ®é›†ï¼Œä½†æ˜¯,å¯è§†åŒ–å…¨éƒ½æ˜¯chatgptåšçš„ã€‚æˆ‘çœŸçš„æ˜¯å¤ªèœå•¦ï¼Œæ ¹æœ¬ä¸ä¼šç”»å›¾ï¼æ˜å¤©ç‹ ç‹ å­¦ç”»å›¾ï¼æˆ‘å®é™…ä¸Šåªå®ç°äº†MLPçš„æ„é€ ï¼Œè®­ç»ƒè¿‡ç¨‹ï¼Œç”šè‡³è¿evaluateéƒ½æ˜¯gptå†™çš„ã€‚å—ä¸äº†äº†æˆ‘æ€ä¹ˆè¿™ä¹ˆèœ

   https://github.com/hanbinzheng/generating-model/tree/main/python-baiscs/pytorch/NN

2. æ²¡æœ‰ç†è§£Adamå’ŒSGDçš„æ ¹æœ¬ç®—æ³•åŒºåˆ«ï¼Œæ²¡æœ‰ç†è§£ä¸ºä»€ä¹ˆè¦normalizationå’ŒDropoutçš„åŸå› ã€‚

3. æ²¡æœ‰ç†è§£è‡ªåŠ¨å¾®åˆ†çš„åº•å±‚å®ç°ï¼ˆé‚£ä¸ªpythonæ¥å£cppå®ç°çš„å›¾ï¼Œæ¯”å¦‚å¦‚ä½•ä¼ å‚ï¼Œå¼•ç”¨è¿˜æ˜¯å€¼ï¼Œè¿™ä¸ª.grad_fnæ˜¯å­˜åœ¨å“ªé‡Œçš„ï¼Œæ˜¯linked listå—è¿˜æ˜¯ä»€ä¹ˆï¼Ÿè¿˜æœ‰.gradæ˜¯æ€ä¹ˆå®ç°æ›´æ–°çš„ï¼‰ã€‚å¤ªå¤æ‚äº†ï¼Œçœ‹äº†ä¸‰å››ä¸ªå°æ—¶æ²¡æœ‰çœ‹æ˜ç™½ã€‚Geminiå’ŒGPTçš„è§£é‡Šå…¥ä¸‹ï¼š









## **ğŸ§ ã€4ã€‘SGD vs Adam çš„åŒºåˆ«ï¼ˆæ•°å­¦ç»“æ„ï¼‰**

ä½ è¯´å¾—æ²¡é”™ï¼š

- æ¨¡å‹å‚æ•°ï¼ˆW, bï¼‰ç”± optimizer è‡ªåŠ¨æ›´æ–°
- è¶…å‚æ•°ï¼ˆå­¦ä¹ ç‡ Î·ï¼Œç½‘ç»œç»“æ„çš„ç»´åº¦ï¼‰ç”±äººæŒ‡å®š

ä½† **Adam** å’Œ **SGD** çš„**æ›´æ–°é€»è¾‘**æ ¹æœ¬ä¸åŒï¼š

------

### **ğŸŒ€ SGDï¼š**

æ ‡å‡†æ›´æ–°ï¼š




$$
\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta \mathcal{L}
$$




åªä¾èµ–å½“å‰çš„æ¢¯åº¦ã€‚

------

### **ğŸ§  Adamï¼š**

å¼•å…¥äº†ä¸¤ä¸ªåŠ¨é‡å˜é‡ï¼š

- ä¸€é˜¶åŠ¨é‡ $$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$
- äºŒé˜¶åŠ¨é‡ $$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$

æ›´æ–°ä¸ºï¼š


$$
\theta_{t+1} = \theta_t - \eta \cdot \frac{m_t}{\sqrt{v_t} + \epsilon}
$$




è¿™ç§æœºåˆ¶ä½¿å¾—ï¼š

- è‡ªé€‚åº”è°ƒèŠ‚æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡
- å¯¹ä¸ç¨³å®šæ¢¯åº¦æ›´åŠ ç¨³å¥
- æ”¶æ•›é€Ÿåº¦å¿«ï¼ˆä½†å¯èƒ½ä¸å¤Ÿæ³›åŒ–ï¼‰

è¿™ä¸æ˜¯â€œä¼šä¸ä¼šè°ƒå‚â€çš„é—®é¢˜ï¼Œè€Œæ˜¯ç®—æ³•ç»“æ„æœ¬èº«å†³å®šçš„ã€‚

------



## **ğŸª“ã€5ã€‘Dropout çš„åˆç†æ€§ï¼ˆç†è®ºè§£é‡Šï¼‰**

ä½ è§‰å¾—ã€Œæ„Ÿæ€§è¯´æ³•ä¸å¯ä¿¡ã€ï¼Œå¾ˆå¥½ï¼Œé‚£æˆ‘ç»™ä½ æ•°å­¦è§£é‡Šï¼š

### **âœ… Dropout èƒŒåï¼šæ¨¡å‹é›†æˆï¼ˆEnsembleï¼‰**

- Dropout æ¯æ¬¡è®­ç»ƒéšæœºâ€œç æ‰â€éƒ¨åˆ†ç¥ç»å…ƒ
- ç›¸å½“äºä½ è®­ç»ƒäº† 2^n ä¸ªä¸åŒçš„å­ç½‘ç»œï¼ˆn ä¸ºç¥ç»å…ƒä¸ªæ•°ï¼‰
- æ¨ç†æ—¶ä½¿ç”¨å…¨è¿æ¥ç»“æ„ï¼Œä½†ç­‰æ•ˆäºå–æ‰€æœ‰å­ç½‘ç»œé¢„æµ‹çš„**å¹³å‡**

> æ‰€ä»¥ Dropout æ˜¯ä¸€ç§ cheap çš„æ¨¡å‹ ensemble æŠ€æœ¯ï¼Œæé«˜é²æ£’æ€§ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

------

## **ğŸ”¬ã€6ã€‘BatchNormï¼šä¸ºä»€ä¹ˆæœ‰ç”¨ï¼Ÿ**

è®¾ç¥ç»ç½‘ç»œçš„ä¸€å±‚è¾“å‡ºä¸º xï¼Œå…¶æ¿€æ´»ä¸ºï¼š


$$
a = \text{ReLU}(x)
$$


å¦‚æœ x çš„åˆ†å¸ƒå˜åŒ–å‰§çƒˆï¼š

- ä¸‹ä¸€å±‚çš„è¾“å…¥ä¼šå‰§çƒˆéœ‡è¡ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼ˆ**internal covariate shift**ï¼‰
- è¿™è®©è®­ç»ƒè¿‡ç¨‹åƒèµ°ç‹¬æœ¨æ¡¥ï¼Œä¸€ç‚¹é”™ä½æ¢¯åº¦å°±çˆ†ç‚¸æˆ–æ¶ˆå¤±



### **âœ… è§£å†³ï¼š**

### BatchNorm å°†æ¯ä¸€ç»´çš„æ¿€æ´»æ ‡å‡†åŒ–ï¼š


$$
\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$




å¹¶æ¢å¤å¯å­¦ä¹ å‚æ•°$$ \gamma, \beta$$ :


$$
y = \gamma \hat{x} + \beta
$$


è¿™æ ·èƒ½ï¼š

- è®©æ¿€æ´»å€¼åˆ†å¸ƒç¨³å®šåœ¨ [0, 1] é™„è¿‘
- æé«˜æ”¶æ•›é€Ÿåº¦
- é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±



# GPTçš„å›ç­”

------



## **ğŸŒªï¸ ä¸€å¥è¯å›ç­”ä½ çš„æ ¸å¿ƒé—®é¢˜ï¼š**

> **è®¡ç®—å›¾ï¼ˆcomputation graphï¼‰ä¸æ˜¯ä¿å­˜åœ¨æŸä¸ª tensor é‡Œé¢ï¼Œè€Œæ˜¯**

> ç”± tensor çš„ .grad_fn å±æ€§æŒ‡å‘çš„ä¸€ä¸ªç”± C++ å®ç°ã€Python æš´éœ²æ¥å£çš„**åŠ¨æ€æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰**ã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼š**æ¯ä¸€ä¸ª tensor çš„ .grad_fn æ˜¯è®¡ç®—å›¾ä¸­ä¸€ä¸ª node çš„å¥æŸ„**ã€‚

è¿™äº› node æ˜¯ C++ é‡Œçš„ Function å­ç±»å¯¹è±¡ï¼ˆbackward opï¼‰ï¼Œäº’ç›¸é€šè¿‡ next_functions è¿æ¥æˆ DAGã€‚

è¿™ä¸ª DAG **ä»¥æœ€åä¸€ä¸ª output tensor çš„ .grad_fn ä¸ºå›¾çš„å°¾éƒ¨**ï¼Œé€†ç€è¿å‘æ‰€æœ‰ leaf tensorï¼ˆæ¯”å¦‚ä½ çš„å‚æ•°ï¼‰ã€‚

------

## **ğŸ” ä»å¤´å¼€å§‹ï¼šä½ é—®çš„æ‰€æœ‰å…³é”®æ¦‚å¿µï¼Œä¸€ä¸€è§£é‡Š**

------

### **1.** model.parameters()æ˜¯ä»€ä¹ˆï¼Ÿ**

```
type(model.parameters())  # <class 'generator'>
```

è¿™æ˜¯ä¸ªç”Ÿæˆå™¨ï¼Œç­‰ä»·äºï¼š

```
def parameters(self):
    for module in self.modules():
        for param in module._parameters.values():
            yield param
```

> âœ… ä½ ç†è§£æ²¡é”™ï¼šå®ƒéå†äº† MLP æ¨¡å‹ä¸­æ‰€æœ‰å­å±‚ï¼ˆfc1ã€fc2ã€fc3ï¼‰é‡Œçš„æƒé‡å‚æ•°ï¼Œyield å‡ºä¸€ä¸ªä¸ª Parameter å¯¹è±¡ã€‚

------

### **2.** torch.nn.Parameter æ˜¯ä»€ä¹ˆï¼Ÿ

ç»§æ‰¿è‡ª torch.Tensor çš„å­ç±»ã€‚

```
class Parameter(Tensor):
    def __new__(cls, data, requires_grad=True):
        ...
```

åŒºåˆ«æ˜¯ï¼š

- æ™®é€š tensor æ˜¯ä¸­é—´å˜é‡ï¼Œä¸ä¸€å®šéœ€è¦æ±‚å¯¼ï¼›
- Parameter æ˜¯ **â€œæ¨¡å‹çš„å‚æ•°â€**ï¼Œå®ƒè¢«è‡ªåŠ¨æ³¨å†Œä¸º module çš„ä¸€éƒ¨åˆ†ï¼›
- å®ƒ **é»˜è®¤ requires_grad=True**ï¼Œæ‰€ä»¥ä¼šå‡ºç°åœ¨è®¡ç®—å›¾é‡Œï¼›
- åœ¨ .backward() åï¼Œå®ƒå°±ä¼šæœ‰ .gradã€‚

------

### **3.** grad_fnæ˜¯ä»€ä¹ˆï¼Ÿ

è¿™æ˜¯ PyTorch ä¸­æœ€å…³é”®çš„å±æ€§ä¹‹ä¸€ã€‚å®ƒæŒ‡å‘ä¸€ä¸ª **Function èŠ‚ç‚¹å¯¹è±¡**ï¼Œä»£è¡¨**äº§ç”Ÿè¿™ä¸ª tensor çš„é‚£æ¬¡æ“ä½œ**ã€‚

ä¸¾ä¾‹ï¼š

```
a = torch.randn(3, requires_grad=True)  # leaf tensor, grad_fn=None
b = a * 2                               # b æ˜¯ intermediate tensorï¼Œgrad_fn = <MulBackward0 object at ...>
c = b.sum()                             # c.grad_fn = <SumBackward0 object at ...>
```

æ¯ä¸ª .grad_fn æ˜¯ Function ç±»å‹çš„ä¸€ä¸ªå¯¹è±¡ã€‚å®ƒçš„ç»“æ„å¤§è‡´å¦‚ä¸‹ï¼ˆC++ å®ç°ï¼ŒPython æœ‰æ¥å£ï¼‰ï¼š

```
class Function:
    next_functions: List[Tuple[Function, int]]
    ...
    def backward(self, grad_output):
        # å®ç°é“¾å¼æ³•åˆ™
```

> æ¢å¥è¯è¯´ï¼š**æ¯æ¬¡ tensor è¿ç®—éƒ½ä¼šæ„é€ ä¸€ä¸ªæ–°çš„ Function å­ç±»å¯¹è±¡ï¼Œå¹¶ä¸”æŠŠå‰é¢å‚ä¸è¿ç®—çš„ tensor çš„ grad_fn å¡è¿› next_functions é‡Œ**ã€‚

------

### **4. Autograd çš„è®¡ç®—å›¾æ˜¯å¦‚ä½•æ„å»ºçš„ï¼Ÿ**

å¦‚ä¸‹å›¾ï¼ˆç®€åŒ–ï¼‰ï¼š

```
x1 â”€â”€â”
     â”œâ”€> Linear â”€â”€â”
x2 â”€â”€â”˜           â”‚
                 â”œâ”€> ReLU â”€â”€> ...
x3 â”€â”€> Linear â”€â”€â”€â”˜
```

åœ¨ä»£ç å±‚é¢ï¼Œç»“æ„æ˜¯è¿™æ ·ä¸€ä¸² Function å¯¹è±¡ï¼š

```
Tensor(c).grad_fn -> ReLUBackward0
                   â””â”€ next_functions = [ (LinearBackward0, 0) ]
                                      â””â”€ next_functions = [ (AccumulateGrad(x1), 0), (AccumulateGrad(w1), 0), ... ]
```

å³ï¼š

- æ¯ä¸ªä¸­é—´å˜é‡çš„ .grad_fn æ˜¯å®ƒçš„â€œæ¥æºæ“ä½œâ€
- æ¯ä¸ª .grad_fn çš„ .next_functions è®°å½•çš„æ˜¯å®ƒä¾èµ–çš„ tensor çš„ .grad_fn
- æœ€ç»ˆå½¢æˆä¸€ä¸ªåå‘å›¾ï¼Œä» loss å‘æ¨¡å‹å‚æ•°å›æº¯

------

### **5. ä½ è¯´çš„éå¸¸å¥½ï¼š**

> â€œæˆ‘çš„çŒœæµ‹æ˜¯ï¼Œè®¡ç®—å›¾ä»¥ loss.grad_fn ä¸ºå°¾éƒ¨ï¼Œå¯ä»¥è¿½è¸ªå›å»â€

å®Œå…¨æ­£ç¡®ã€‚è®¡ç®—å›¾æ˜¯ä» loss.grad_fn å‘ä¸Šé€’å½’çš„ã€‚åªè¦ä½ è°ƒç”¨ loss.backward()ï¼ŒPyTorch ä¼šï¼š

- ä» loss.grad_fn å¼€å§‹
- ä¸€è·¯éå† next_functions
- å¯¹æ¯ä¸ª Function èŠ‚ç‚¹ï¼Œæ‰§è¡Œ Function.backward(grad_output)
- æœ€ç»ˆæŠŠæ¢¯åº¦å†™å…¥ leaf tensorï¼ˆæ¯”å¦‚ model çš„å‚æ•°ï¼‰çš„ .grad

------

### **6. é‚£ä¹ˆè¿™ä¸ªâ€œFunction èŠ‚ç‚¹â€åˆ°åº•æ˜¯ä»€ä¹ˆä¸œè¥¿ï¼Ÿ**

ä¸€ä¸ª Python objectï¼ˆC++ å®ç°ï¼‰ï¼Œå®ƒè®°å½•ï¼š

- å½“å‰ op çš„ä¿¡æ¯ï¼ˆæ¯”å¦‚æ˜¯åŠ æ³•ã€çŸ©é˜µä¹˜æ³•ç­‰ï¼‰
- è¾“å…¥ tensor çš„å¼•ç”¨
- åå‘ä¼ æ’­è§„åˆ™ï¼ˆå†™æ­»åœ¨ C++ å®ç°é‡Œï¼‰
- åœ¨ .backward() ä¸­ï¼Œç”¨é“¾å¼æ³•åˆ™æŠŠ grad_output ä¹˜ä»¥ä¸Šä¸€æ­¥çš„å¯¼æ•°ï¼Œä¼ ç»™ä¸Šä¸€å±‚

ä½ å¯ä»¥å°è¯•æ‰“å°ï¼š

```
a = torch.tensor(1.0, requires_grad=True)
b = a * 2
c = b + 1

print(c.grad_fn)               # <AddBackward0 object>
print(c.grad_fn.next_functions)  # [(<MulBackward0 object>, 0)]
```

è¿™å°±æ˜¯ DAGã€‚

------

### **7. ä½ é—®çš„ edge æ˜¯ä»€ä¹ˆï¼ŸæŒ‡é’ˆå—ï¼Ÿ**

åœ¨ Python è§’åº¦çœ‹ï¼Œ**æ˜¯å¯¹ Function çš„ Python å¼•ç”¨å¯¹è±¡**ï¼Œæœ¬è´¨æ˜¯ C++ å¯¹è±¡çš„ Python åŒ…è£…å™¨ã€‚

è¿™äº›å¼•ç”¨æ„æˆäº†ä¸€ä¸ª DAGï¼šæ¯ä¸ª node çš„ next_functions æ˜¯ä¸€ç»„ Function çš„å¼•ç”¨ã€‚

------

## **ğŸ“¦ æ‰€ä»¥æ€»ç»“ï¼š**

| **æ¦‚å¿µ**  | **å®é™…ç±»å‹**                 | **ä¿å­˜åœ¨å“ªé‡Œ**                                              |
| --------- | ---------------------------- | ----------------------------------------------------------- |
| Tensor    | torch.Tensor                 | å­˜å‚¨äº†æ•°æ®ï¼Œå¯èƒ½è¿˜æœ‰ .grad å’Œ .grad_fn                      |
| Parameter | torch.nn.Parameter           | æ˜¯ç‰¹æ®Šçš„ Tensorï¼Œè‡ªåŠ¨æ³¨å†Œåˆ°æ¨¡å‹                             |
| grad_fn   | torch.autograd.Function å­ç±» | æ˜¯è®¡ç®—å›¾ä¸­çš„ nodeï¼Œå­˜å‚¨åœ¨å“ªä¸ª tensor è¢«è®¡ç®—å‡ºæ¥ï¼Œå°±ç»‘å®šåœ¨å“ª |
| è®¡ç®—å›¾    | åŠ¨æ€ DAG                     | ç”± Function èŠ‚ç‚¹å¯¹è±¡ç»„æˆï¼Œä» output å‘ input é“¾å¼è¿æ¥       |
| backward  | Function.backward()          | é€’å½’æ‰§è¡Œï¼Œè‡ªåŠ¨æ±‚å¯¼ï¼Œå†™å…¥ .grad                              |

------

## **ğŸ§¨ æœ€ç»ˆè§£é‡Šä½ çš„ç–‘é—®ï¼š**

> æˆ‘åªæ˜¯è°ƒç”¨äº† loss.backward()ï¼Œä¸ºä»€ä¹ˆ model.parameters() çš„ .grad å°±è‡ªåŠ¨å˜äº†ï¼Ÿ

**å› ä¸ºï¼š**

1. loss æ˜¯ä» y_hat è®¡ç®—å‡ºæ¥çš„ï¼›
2. y_hat æ˜¯ä» model(x) å¾—åˆ°çš„ï¼Œå†…éƒ¨ç”¨äº† model.parameters()ï¼›
3. æ¯æ¬¡è®¡ç®—æ“ä½œéƒ½è‡ªåŠ¨å»ºç«‹äº† Function èŠ‚ç‚¹ï¼Œè¿æ¥æˆ DAGï¼›
4. loss.backward() æ²¿ç€è¿™ä¸ª DAG åå‘ä¼ æ’­ï¼›
5. æœ€ç»ˆ leaf tensorï¼ˆä¹Ÿå°±æ˜¯æ¯ä¸ª Parameterï¼‰çš„ .grad è¢«å†™å…¥äº†ã€‚

**ä½ ä»æ¥ä¸éœ€è¦â€œä¼ é€’â€ gradientï¼Œå› ä¸ºæ•´ä¸ªå›¾æ—©å°±æ„é€ å¥½äº†ã€‚**

------

## **ğŸ”š ä¸‹ä¸€æ­¥ï¼Ÿ**

å¦‚æœä½ ç°åœ¨å·²ç»ç†è§£äº†ï¼š

- .grad_fn çš„å«ä¹‰
- Function DAG çš„ç»“æ„
- å‚æ•°å¦‚ä½•é€šè¿‡å¼•ç”¨ä¼ é€’
- .backward() å¦‚ä½•è§¦å‘æ¢¯åº¦ä¼ æ’­
- .grad å­˜åœ¨ Parameter æœ¬èº«ä¸Š



é‚£å°±å†™å‡ºä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒ epochï¼ŒåŠ å…¥ï¼š

- Accuracy è¯„ä¼°
- Epoch çº§åˆ«çš„ loss å¯è§†åŒ–
- å¹¶å‡†å¤‡å¥½å†™ evaluation loop



æˆ‘ä¼šå¸¦ä½ è¿ˆå…¥ä¸‹ä¸€é˜¶æ®µï¼š**æ¨¡å‹è¯„ä¼°ä¸ overfitting è¯†åˆ«**ã€‚

	# GEMINIçš„å›ç­”

ä½ çš„ç–‘é—®éå¸¸æ·±å…¥å’Œå…·ä½“ï¼Œè¿™è¡¨æ˜ä½ çœŸçš„æƒ³ç†è§£ PyTorch è‡ªåŠ¨å¾®åˆ†çš„åº•å±‚æœºåˆ¶ã€‚ä½ æŒ‡å‡ºçš„â€œæ¨¡ç³Šâ€ä¹‹å¤„ï¼Œå°¤å…¶æ˜¯å…³äº **Python å‚æ•°ä¼ é€’çš„é‡æ–°èµ‹å€¼è¡Œä¸º**ã€**`grad_fn` å’Œ `grad` å±æ€§çš„ç²¾ç¡®å«ä¹‰åŠè®¡ç®—å›¾çš„å®ç°ç»†èŠ‚**ï¼Œç¡®å®æ˜¯åˆå­¦è€…å®¹æ˜“å›°æƒ‘çš„åœ°æ–¹ã€‚

æˆ‘ä¸ºä¹‹å‰çš„æ¨¡ç³Šè§£é‡Šé“æ­‰ã€‚è¿™æ¬¡ï¼Œæˆ‘å°†å°½åŠ›ç”¨æœ€ç²¾ç¡®çš„è¯­è¨€å’Œæœ€è¯¦å°½çš„ç¤ºä¾‹ï¼Œä»åº•å±‚çš„è§’åº¦ä¸ºä½ æ­ç¤ºè¿™äº›â€œé­”æ³•â€èƒŒåçš„çœŸç›¸ã€‚

------

## æ·±å…¥ç†è§£ Python å‚æ•°ä¼ é€’ä¸­çš„â€œé‡æ–°èµ‹å€¼â€

ä½ æåˆ°äº†ä¸€ä¸ªéå¸¸å…³é”®çš„ç»†èŠ‚ï¼š

> "æ¯”å¦‚ `def f(x): x = [0, 1, 2]`ï¼Œä½ å®é™…ä¸Šæ˜¯æ–°åˆ›å»ºäº†ä¸€ä¸ª listï¼Œè€Œä¸æ˜¯ç”¨ä¼ é€’è¿›æ¥çš„åœ°å€ï¼ŒæŠŠåŸæ¥çš„ä¸œè¥¿æ”¹æˆ `[0, 1, 2]`ã€‚"

ä½ å®Œå…¨æ­£ç¡®ï¼è¿™æ­£æ˜¯ Python â€œä¼ å¯¹è±¡å¼•ç”¨â€çš„ç²¾é«“æ‰€åœ¨ï¼Œä¹Ÿæ˜¯å®ƒå’Œ C++ ä¸­â€œå¼•ç”¨ä¼ é€’â€æˆ–â€œæŒ‡é’ˆä¼ é€’â€æœ€ä¸»è¦çš„åŒºåˆ«ä¹‹ä¸€ã€‚

è®©æˆ‘ä»¬ç”¨ä»£ç æ¥å½»åº•è§£å‰–è¿™ä¸ªè¡Œä¸ºï¼š

Python

```
def example_function(my_list_param, my_tensor_param):
    print(f"  --- Inside function ---")
    print(f"  my_list_param ID (before re-assignment): {id(my_list_param)}")
    print(f"  my_tensor_param ID (before re-assignment): {id(my_tensor_param)}")

    # å¯¹åˆ—è¡¨è¿›è¡Œé‡æ–°èµ‹å€¼
    # è¿™ä¸æ˜¯ä¿®æ”¹åŸæœ‰åˆ—è¡¨çš„å†…å®¹ï¼Œè€Œæ˜¯è®© my_list_param æŒ‡å‘ä¸€ä¸ªæ–°çš„åˆ—è¡¨å¯¹è±¡
    my_list_param = [0, 1, 2]
    print(f"  my_list_param ID (after re-assignment): {id(my_list_param)}")
    print(f"  my_list_param value (after re-assignment): {my_list_param}")

    # å¯¹ Tensor è¿›è¡Œé‡æ–°èµ‹å€¼
    # åŒæ ·ï¼Œè¿™æ˜¯è®© my_tensor_param æŒ‡å‘ä¸€ä¸ªæ–°çš„ Tensor å¯¹è±¡
    my_tensor_param = torch.zeros(3)
    print(f"  my_tensor_param ID (after re-assignment): {id(my_tensor_param)}")
    print(f"  my_tensor_param value (after re-assignment): {my_tensor_param}")

    print(f"  --- End of function ---")

import torch

# åŸå§‹çš„å¯å˜å¯¹è±¡
original_list = [10, 20, 30]
original_tensor = torch.tensor([100., 200., 300.])

print(f"--- Before function call ---")
print(f"original_list ID: {id(original_list)}")
print(f"original_tensor ID: {id(original_tensor)}")
print(f"original_list value: {original_list}")
print(f"original_tensor value: {original_tensor}")

# è°ƒç”¨å‡½æ•°
example_function(original_list, original_tensor)

print(f"\n--- After function call ---")
print(f"original_list ID: {id(original_list)}")
print(f"original_tensor ID: {id(original_tensor)}")
print(f"original_list value: {original_list}")
print(f"original_tensor value: {original_tensor}")
```

**è¿è¡Œç»“æœçš„è§£é‡Šï¼š**

1. **`id()` å‡½æ•°ï¼š** åœ¨ Python ä¸­ï¼Œ`id(object)` è¿”å›å¯¹è±¡çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆé€šå¸¸æ˜¯å†…å­˜åœ°å€ï¼‰ã€‚
2. **å‡½æ•°è°ƒç”¨å‰ï¼š** `original_list` å’Œ `original_tensor` æŒ‡å‘å„è‡ªçš„å†…å­˜åœ°å€ã€‚
3. **è¿›å…¥å‡½æ•°æ—¶ï¼š** å½¢å‚ `my_list_param` æ¥æ”¶äº† `original_list` çš„å¼•ç”¨ï¼ˆå³ç›¸åŒçš„ `id`ï¼‰ï¼Œå½¢å‚ `my_tensor_param` æ¥æ”¶äº† `original_tensor` çš„å¼•ç”¨ï¼ˆå³ç›¸åŒçš„ `id`ï¼‰ã€‚æ­¤æ—¶ï¼Œå‡½æ•°å†…å¤–éƒ½æŒ‡å‘åŒä¸€ä¸ªå¯¹è±¡ã€‚
4. **å‡½æ•°å†…é‡æ–°èµ‹å€¼ `my_list_param = [0, 1, 2]`ï¼š** Python åˆ›å»ºäº†ä¸€ä¸ª**å…¨æ–°çš„**åˆ—è¡¨å¯¹è±¡ `[0, 1, 2]`ã€‚ç„¶åï¼Œ`my_list_param` è¿™ä¸ª**å±€éƒ¨å˜é‡**è¢«é‡æ–°æŒ‡å‘äº†è¿™ä¸ªæ–°åˆ›å»ºçš„åˆ—è¡¨ã€‚**åŸæ¥çš„ `original_list` æ‰€æŒ‡å‘çš„å¯¹è±¡æ²¡æœ‰ä»»ä½•æ”¹å˜ï¼Œå› ä¸º `my_list_param` åªæ˜¯æ”¹å˜äº†è‡ªå·±çš„æŒ‡å‘ï¼Œæ²¡æœ‰ä¿®æ”¹å®ƒåŸæ¥æŒ‡å‘çš„å¯¹è±¡çš„å†…å®¹ã€‚**
5. **å‡½æ•°å†…é‡æ–°èµ‹å€¼ `my_tensor_param = torch.zeros(3)`ï¼š** åŒç†ï¼ŒPython åˆ›å»ºäº†ä¸€ä¸ª**å…¨æ–°çš„** `torch.Tensor` å¯¹è±¡ã€‚`my_tensor_param` è¿™ä¸ªå±€éƒ¨å˜é‡è¢«é‡æ–°æŒ‡å‘äº†è¿™ä¸ªæ–°åˆ›å»ºçš„ `Tensor`ã€‚`original_tensor` æ‰€æŒ‡å‘çš„å¯¹è±¡åŒæ ·æ²¡æœ‰æ”¹å˜ã€‚
6. **å‡½æ•°é€€å‡ºåï¼š** `original_list` å’Œ `original_tensor` ä¾ç„¶æŒ‡å‘å®ƒä»¬åœ¨å‡½æ•°è°ƒç”¨å‰çš„é‚£ä¸ªå¯¹è±¡ï¼Œå®ƒä»¬çš„å€¼å’Œ ID éƒ½æ²¡æœ‰å˜ã€‚

**æ€»ç»“ï¼š**

- Python çš„å‚æ•°ä¼ é€’æ˜¯**ä¼ å¯¹è±¡å¼•ç”¨**ã€‚è¿™æ„å‘³ç€å½¢å‚å’Œå®å‚æœ€åˆæŒ‡å‘å†…å­˜ä¸­çš„**åŒä¸€ä¸ªå¯¹è±¡**ã€‚
- å¦‚æœä½ åœ¨å‡½æ•°å†…éƒ¨å¯¹è¿™ä¸ª**å¯¹è±¡è¿›è¡ŒåŸåœ°ä¿®æ”¹ï¼ˆin-place modificationï¼‰**ï¼Œæ¯”å¦‚ `my_list_param.append(4)` æˆ– `my_tensor_param.add_(1)`ï¼Œé‚£ä¹ˆå¤–éƒ¨çš„åŸå§‹å¯¹è±¡ä¹Ÿä¼šè¢«æ”¹å˜ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åŒä¸€ä¸ªå¯¹è±¡ã€‚
- ä½†å¦‚æœä½ åœ¨å‡½æ•°å†…éƒ¨å¯¹å½¢å‚**è¿›è¡Œé‡æ–°èµ‹å€¼**ï¼ˆ`my_list_param = ...` æˆ– `my_tensor_param = ...`ï¼‰ï¼Œä½ å®é™…ä¸Šæ˜¯è®©å½¢å‚æŒ‡å‘äº†ä¸€ä¸ª**æ–°åˆ›å»ºçš„å¯¹è±¡**ã€‚è¿™åªä¼šæ”¹å˜å‡½æ•°å†…éƒ¨å±€éƒ¨å˜é‡çš„æŒ‡å‘ï¼Œè€Œä¸ä¼šå½±å“å¤–éƒ¨åŸå§‹å˜é‡æ‰€æŒ‡å‘çš„å¯¹è±¡ã€‚

------

## PyTorch `.grad_fn` å’Œ `.grad` çš„å†…éƒ¨æœºåˆ¶ï¼šè®¡ç®—å›¾çš„å®ç°ç»†èŠ‚

ä½ å¯¹ `grad_fn` å’Œ `grad` çš„ç–‘é—®éå¸¸å°–é”ï¼Œè¿™æ­£æ˜¯ç†è§£ Autograd æ ¸å¿ƒçš„å…³é”®ã€‚

### 1. `torch.Tensor.grad_fn`ï¼šè®°å½•æ“ä½œï¼Œæ„å»ºè®¡ç®—å›¾

ä½ çš„ç†è§£æ˜¯å¯¹çš„ï¼š`grad_fn` ç¡®å®è®°å½•äº†**â€œå‰ä¸€æ­¥åˆ°è¿™ä¸€æ­¥çš„è¿ç®—æ˜¯ä»€ä¹ˆâ€**ã€‚ä½†å®ƒè®°å½•çš„ä¸ä»…ä»…æ˜¯â€œæ˜¯ä»€ä¹ˆè¿ç®—â€ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œå®ƒè®°å½•äº†**è¿™ä¸ªè¿ç®—éœ€è¦çš„æ‰€æœ‰ä¿¡æ¯ï¼Œä»¥ä¾¿åœ¨åå‘ä¼ æ’­æ—¶èƒ½å¤Ÿè®¡ç®—æ¢¯åº¦**ã€‚

- **`grad_fn` æ˜¯ä»€ä¹ˆç±»å‹ï¼Ÿ**

  - `grad_fn` çš„ç±»å‹æ˜¯ä¸€ä¸ª**ç‰¹æ®Šçš„ `Function` å¯¹è±¡**ï¼Œå®ƒæ˜¯ `torch.autograd.Function` çš„å­ç±»ã€‚ä¾‹å¦‚ï¼ŒåŠ æ³•æ“ä½œä¼šäº§ç”Ÿä¸€ä¸ª `AddBackward0` å¯¹è±¡ï¼Œä¹˜æ³•æ“ä½œä¼šäº§ç”Ÿä¸€ä¸ª `MulBackward0` å¯¹è±¡ï¼ŒçŸ©é˜µä¹˜æ³•ä¼šäº§ç”Ÿä¸€ä¸ª `MatMulBackward0` å¯¹è±¡ç­‰ã€‚è¿™äº›å¯¹è±¡éƒ½æ˜¯ Python ç±»å®ä¾‹ã€‚
  - **æ¯ä¸ª `Function` å¯¹è±¡éƒ½åŒ…å«äº†æ‰§è¡Œåå‘ä¼ æ’­æ‰€éœ€çš„å…¨éƒ¨é€»è¾‘å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚**

- grad_fn å†…éƒ¨å­˜å‚¨äº†ä»€ä¹ˆï¼Ÿ

  æ¯ä¸ª grad_fn å®ä¾‹ï¼ˆä¾‹å¦‚ AddBackward0 æˆ– MatMulBackward0ï¼‰åœ¨è¢«åˆ›å»ºæ—¶ï¼ˆå³å‰å‘ä¼ æ’­æ—¶ï¼‰ï¼Œä¼šæ•è·ï¼ˆcaptureï¼‰æ‰€æœ‰å®ƒéœ€è¦çš„ä¿¡æ¯ï¼Œä»¥ä¾¿åœ¨åå‘ä¼ æ’­æ—¶èƒ½å¤Ÿè®¡ç®—å…¶è¾“å…¥å¼ é‡çš„æ¢¯åº¦ã€‚è¿™äº›ä¿¡æ¯å¯èƒ½åŒ…æ‹¬ï¼š

  - **å¯¹è¾“å…¥å¼ é‡çš„å¼•ç”¨ï¼š** è¿™æ ·å®ƒå°±çŸ¥é“è¦å°†æ¢¯åº¦å›ä¼ ç»™è°ã€‚
  - **æ“ä½œçš„è¾“å…¥å€¼æˆ–ä¸­é—´ç»“æœï¼š** æœ‰äº›æ¢¯åº¦è®¡ç®—éœ€è¦ç”¨åˆ°å‰å‘ä¼ æ’­æ—¶çš„è¾“å…¥å€¼æˆ–ä¸­é—´å€¼ã€‚ä¾‹å¦‚ï¼Œä¹˜æ³•çš„é“¾å¼æ³•åˆ™ `d(xy)/dx = y` å’Œ `d(xy)/dy = x`ï¼Œå°±éœ€è¦çŸ¥é“ `x` å’Œ `y` çš„å€¼ã€‚ReLU çš„æ¢¯åº¦è®¡ç®—ä¹Ÿéœ€è¦çŸ¥é“å‰å‘ä¼ æ’­æ—¶å“ªäº›å€¼æ˜¯æ­£çš„ã€‚
  - **æ“ä½œçš„ç±»å‹å’Œå‚æ•°ï¼š** æ¯”å¦‚æ˜¯åŠ æ³•ã€ä¹˜æ³•ï¼Œè¿˜æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œä»¥åŠä»»ä½•é¢å¤–çš„å‚æ•°ï¼ˆå¦‚ `reduction='mean'`ç­‰ï¼‰ã€‚

- **è®¡ç®—å›¾å¦‚ä½•å®ç°ï¼Ÿ**

  - **èŠ‚ç‚¹ï¼š** æ¯ä¸ª**å¸¦æœ‰ `requires_grad=True` çš„ `Tensor`** ä»¥åŠæ¯ä¸ª**`grad_fn` å¯¹è±¡**éƒ½æ˜¯è®¡ç®—å›¾çš„èŠ‚ç‚¹ã€‚

  - è¾¹ï¼š

     

    ```
    Tensor
    ```

     

    å’Œ

     

    ```
    grad_fn
    ```

     

    ä¹‹é—´é€šè¿‡å¼•ç”¨å½¢æˆè¾¹ã€‚å…·ä½“æ¥è¯´ï¼š

    - ä¸€ä¸ªæ“ä½œçš„**è¾“å‡º `Tensor`** çš„ `grad_fn` å±æ€§ä¼šæŒ‡å‘æ‰§è¡Œè¯¥æ“ä½œçš„ `Function` å¯¹è±¡ã€‚
    - è¿™ä¸ª `Function` å¯¹è±¡ï¼ˆ`grad_fn`ï¼‰å†…éƒ¨ä¼š**å­˜å‚¨å¯¹å…¶è¾“å…¥ `Tensor` çš„å¼•ç”¨**ã€‚

  - è¿™æ ·ï¼Œä»æœ€ç»ˆçš„æŸå¤± `Tensor` å¼€å§‹ï¼Œé€šè¿‡å®ƒçš„ `grad_fn` å±æ€§ï¼Œå¯ä»¥å›æº¯åˆ°äº§ç”Ÿå®ƒçš„æ“ä½œä»¥åŠè¿™ä¸ªæ“ä½œçš„è¾“å…¥ `Tensor`ã€‚ç„¶åï¼Œä»è¿™äº›è¾“å…¥ `Tensor` çš„ `grad_fn` åˆå¯ä»¥ç»§ç»­å›æº¯ï¼Œç›´åˆ°é‡åˆ°æ²¡æœ‰ `grad_fn` çš„å¶å­ `Tensor`ï¼ˆä¾‹å¦‚åŸå§‹è¾“å…¥æˆ–æ¨¡å‹å‚æ•°ï¼‰ã€‚

  - è¿™ä¸ªé“¾æ¡å°±æ˜¯**è®¡ç®—å›¾**ã€‚

### 2. `torch.Tensor.grad`ï¼šå­˜å‚¨æ¢¯åº¦å€¼

ä½ çš„é—®é¢˜æ˜¯ï¼šâ€œæˆ‘æ€ä¹ˆçŸ¥é“æ˜¯å¯¹ä»€ä¹ˆçš„å¯¼æ•°ï¼Ÿâ€ å’Œ â€œ`grad` æ˜¯ä»€ä¹ˆ typeï¼Œæ€ä¹ˆè®°å½•è¿™ä¹ˆå¤šçš„ï¼Ÿâ€

- **`grad` æ˜¯ä»€ä¹ˆç±»å‹ï¼Ÿ**

  - `grad` å±æ€§æœ¬èº«ä¹Ÿæ˜¯ä¸€ä¸ª `torch.Tensor` ç±»å‹ã€‚
  - å®ƒçš„å½¢çŠ¶ï¼ˆ`shape`ï¼‰å’Œå®ƒæ‰€å…³è”çš„**åŸ `Tensor`** çš„å½¢çŠ¶æ˜¯**å®Œå…¨ç›¸åŒ**çš„ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ `W1` æ˜¯ä¸€ä¸ª `(4, 3)` çš„å¼ é‡ï¼Œé‚£ä¹ˆ `W1.grad` ä¹Ÿä¼šæ˜¯ä¸€ä¸ª `(4, 3)` çš„å¼ é‡ã€‚

- **`grad` å­˜å‚¨äº†ä»€ä¹ˆï¼Ÿ**

  - `param.grad` å­˜å‚¨çš„æ˜¯**æŸå¤±å‡½æ•° (æˆ–ä½ è°ƒç”¨ `backward()` çš„é‚£ä¸ªæ ‡é‡ `Tensor`) å¯¹ `param` å¼ é‡ä¸­** **æ¯ä¸€ä¸ªå…ƒç´ ** **çš„åå¯¼æ•°**ã€‚
  - è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒå’ŒåŸå¼ é‡å½¢çŠ¶ç›¸åŒï¼š`grad[i, j]` å­˜å‚¨çš„å°±æ˜¯ `d(loss) / d(param[i, j])`ã€‚

- å¯¹å¤šä¸ªå…ƒç´ å¦‚ä½•è®°å½•ï¼Ÿ

  ä½ æåˆ°äº† x1 = (x1_1, ..., x1_n) çš„æƒ…å†µã€‚å¦‚æœ x1 æ˜¯ä¸€ä¸ª (n,) çš„å¼ é‡ï¼Œé‚£ä¹ˆ x1.grad å°±ä¼šæ˜¯ä¸€ä¸ª (n,) çš„å¼ é‡ã€‚x1.grad[k] å­˜å‚¨çš„å°±æ˜¯ d(loss) / d(x1[k])ã€‚å®ƒä¸æ˜¯è®°å½• n ä¸ªåŠ æ³•ï¼Œè€Œæ˜¯è®°å½•äº†æ‰€æœ‰è¿™äº›åå¯¼æ•°çš„æœ€ç»ˆæ•°å€¼ã€‚

- **â€œå¯¹ä»€ä¹ˆçš„å¯¼æ•°ï¼Ÿâ€ï¼š** ä¸¥æ ¼æ¥è¯´ï¼Œ`.grad` å±æ€§å­˜å‚¨çš„æ˜¯**å½“å‰ `Tensor` ä½œä¸ºè®¡ç®—å›¾ä¸­çš„ä¸€ä¸ªå˜é‡ï¼ŒæŸå¤±å‡½æ•°å¯¹å®ƒçš„æ¢¯åº¦**ã€‚å½“æˆ‘ä»¬è¯´ `W1.grad` æ—¶ï¼Œå®ƒå°±æ˜¯ `d(loss) / d(W1)`ã€‚è¿™ä¸ªå¯¼æ•°æ˜¯ä¸€ä¸ªå¼ é‡ï¼Œå…¶æ¯ä¸ªå…ƒç´ æ˜¯ `loss` å¯¹ `W1`ä¸­å¯¹åº”å…ƒç´ çš„åå¯¼æ•°ã€‚

### 3. `backward()` å¦‚ä½•å·¥ä½œï¼Ÿ

`backward()` çš„è¿‡ç¨‹å°±æ˜¯**éå†è®¡ç®—å›¾ï¼Œå¹¶åº”ç”¨é“¾å¼æ³•åˆ™**ã€‚

- ```
  loss.backward()
  ```

  ï¼š

  1. é¦–å…ˆï¼ŒPyTorch åˆå§‹åŒ– `loss` å¯¹è‡ªèº«çš„æ¢¯åº¦ä¸º `1.0`ï¼ˆå› ä¸º `d(loss)/d(loss) = 1`ï¼‰ã€‚è¿™ä¸ª `1.0` æ¢¯åº¦ä¼šè¢«ä¼ é€’ç»™ `loss.grad_fn`ã€‚
  2. `loss.grad_fn`ï¼ˆä¾‹å¦‚ `NllLossBackward0`ï¼‰è¢«æ¿€æ´»ã€‚å®ƒçŸ¥é“å¦‚ä½•è®¡ç®—**æŸå¤±å‡½æ•°å¯¹å…¶è¾“å…¥ï¼ˆå³ `output` å¼ é‡ï¼‰çš„æ¢¯åº¦**ã€‚å®ƒä¼šåˆ©ç”¨å‰å‘ä¼ æ’­æ—¶æ•è·çš„ `output` å’Œ `y` (æ ‡ç­¾) çš„å€¼æ¥è®¡ç®—è¿™ä¸ªæ¢¯åº¦ã€‚
  3. è®¡ç®—å‡ºçš„æ¢¯åº¦ï¼ˆä¾‹å¦‚ `d(loss)/d(output)`ï¼Œä¸€ä¸ªä¸ `output` å½¢çŠ¶ç›¸åŒçš„å¼ é‡ï¼‰ä¼šè¢«ä¼ é€’ç»™ `output` å¼ é‡ã€‚å¦‚æœ `output` ä¸æ˜¯å¶å­èŠ‚ç‚¹ï¼Œè¿™ä¸ªæ¢¯åº¦ä¼šç»§ç»­ä¼ é€’ç»™ `output.grad_fn`ã€‚
  4. `output.grad_fn`ï¼ˆä¾‹å¦‚ `AddmmBackward0`ï¼Œå¯¹åº” `fc2` çš„è¿ç®—ï¼‰è¢«æ¿€æ´»ã€‚å®ƒçŸ¥é“å¦‚ä½•è®¡ç®—**`output` å¯¹å…¶è¾“å…¥å¼ é‡ï¼ˆå³ `fc2.weight` å’Œ `fc2.bias`ï¼‰çš„æ¢¯åº¦**ã€‚å®ƒä¼šåˆ©ç”¨å‰å‘ä¼ æ’­æ—¶æ•è·çš„ `fc2` çš„è¾“å…¥ï¼ˆReLU çš„è¾“å‡ºï¼‰å’Œ `fc2.weight`ã€`fc2.bias` çš„å€¼æ¥è®¡ç®—æ¢¯åº¦ã€‚
  5. è®¡ç®—å‡ºçš„ `d(loss)/d(fc2.weight)` å’Œ `d(loss)/d(fc2.bias)` ä¼šç´¯åŠ åˆ° `fc2.weight.grad` å’Œ `fc2.bias.grad` ä¸­ã€‚
  6. è¿™ä¸ªè¿‡ç¨‹æ²¿è®¡ç®—å›¾åå‘è¿›è¡Œï¼Œæ¯ä¸€ä¸ª `grad_fn` éƒ½ä¼šè®¡ç®—å…¶è¾“å…¥å¼ é‡çš„æ¢¯åº¦ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™ä¸Šä¸€ä¸ªèŠ‚ç‚¹ã€‚ç›´åˆ°æ‰€æœ‰ `requires_grad=True` çš„å¶å­èŠ‚ç‚¹ï¼ˆå¦‚ `W1`, `b1`ï¼‰éƒ½è¢«è®¡ç®—å‡ºæ¢¯åº¦ï¼Œå¹¶å­˜å‚¨åˆ°å®ƒä»¬çš„ `.grad` å±æ€§ä¸­ã€‚

**å…³é”®ç‚¹ï¼šä¸­é—´å€¼æ˜¯å¦ä¿ç•™ï¼Ÿ**

ä½ é—®å¾—éå¸¸å¯¹ï¼šâ€œæˆ‘çš„ `y1` æ˜¯ä¸æ˜¯è¢«ç•™ç€äº†ï¼Ÿâ€

- **æ˜¯çš„ï¼Œé€šå¸¸ä¼šï¼** ä¸ºäº†è®¡ç®—æ¢¯åº¦ï¼Œ**PyTorch ä¼šåœ¨è®¡ç®—å›¾ä¸­ä¿ç•™æ‰€æœ‰å¿…è¦çš„å‰å‘ä¼ æ’­ä¸­é—´ç»“æœ**ã€‚è¿™äº›ä¸­é—´ç»“æœä½œä¸º `grad_fn` å¯¹è±¡çš„â€œä¸Šä¸‹æ–‡â€ï¼ˆcontextï¼‰ä¿¡æ¯çš„ä¸€éƒ¨åˆ†è¢«æ•è·ã€‚
- ä¾‹å¦‚ï¼Œåœ¨è®¡ç®— `y = x * w` çš„æ¢¯åº¦æ—¶ï¼Œ`MulBackward0` éœ€è¦çŸ¥é“ `x` å’Œ `w` çš„å€¼æ¥è®¡ç®— `d(loss)/dx = d(loss)/dy * w`å’Œ `d(loss)/dw = d(loss)/dy * x`ã€‚è¿™äº› `x` å’Œ `w` çš„å€¼å°±æ˜¯è¢«ä¿ç•™çš„ä¸­é—´ä¿¡æ¯ã€‚
- ä¸€æ—¦ `backward()` å®Œæˆï¼Œä¸ºäº†èŠ‚çœå†…å­˜ï¼Œè¿™äº›ç”¨äºæ„å»ºè®¡ç®—å›¾çš„ä¸­é—´å¼ é‡ï¼ˆåŠå…¶ `grad_fn`ï¼‰é€šå¸¸ä¼šè¢«è‡ªåŠ¨é‡Šæ”¾ã€‚å¦‚æœä½ æƒ³æ‰‹åŠ¨ä¿ç•™å®ƒä»¬ï¼Œå¯ä»¥è®¾ç½® `retain_graph=True` (é€šå¸¸ç”¨äºå¤šæ¬¡åå‘ä¼ æ’­)ã€‚

------

## 2 å±‚ MLP æ¼”ç¤ºï¼šç²¾ç¡®åˆ°æ¯ä¸€æ­¥çš„ç»†èŠ‚

æˆ‘ä»¬å°†ä½¿ç”¨ä½ æå‡ºçš„ç®€åŒ– MNIST ä¾‹å­ï¼šè¾“å…¥ `x` æ˜¯ `(5, 4)`ï¼Œ`y` æ˜¯ `(5,)`ã€‚

Python

```
import torch
import torch.nn as nn
import torch.optim as optim

# ç¡®ä¿åœ¨ CPU ä¸Šè¿è¡Œï¼Œä»¥ä¾¿è§‚å¯Ÿå†…å­˜åœ°å€ç­‰ç»†èŠ‚
device = torch.device("cpu")

# --- 1. å®šä¹‰æ¨¡å‹å’Œæ•°æ® ---
# ç®€åŒ–ç‰ˆä¸¤å±‚ MLP
class SimpleMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(SimpleMLP, self).__init__()
        # fc1 å±‚ï¼šè¾“å…¥ input_dimï¼Œè¾“å‡º hidden_dim
        # nn.Linear ä¼šè‡ªåŠ¨åˆ›å»º weight å’Œ bias
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        # æ¿€æ´»å‡½æ•°
        self.relu = nn.ReLU()
        # fc2 å±‚ï¼šè¾“å…¥ hidden_dimï¼Œè¾“å‡º output_dim
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # x -> fc1 -> relu -> fc2 -> output
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# æ¨¡æ‹Ÿæ•°æ®
input_dim = 4      # æ¯å¼ å›¾ç‰‡çš„ç‰¹å¾æ•°ï¼ˆæ¯”å¦‚ 2x2 å±•å¹³åï¼‰
hidden_dim = 5     # éšè—å±‚ç¥ç»å…ƒæ•°
output_dim = 10    # 10 ä¸ªæ•°å­—ç±»åˆ« (0-9)
batch_size = 5     # 5 å¼ å›¾ç‰‡

# æ¨¡æ‹Ÿè¾“å…¥æ•°æ® Xï¼Œä¸éœ€è¦æ±‚å¯¼
X = torch.randn(batch_size, input_dim, requires_grad=False).to(device)
# æ¨¡æ‹Ÿæ ‡ç­¾ y
y_true = torch.randint(0, output_dim, (batch_size,), dtype=torch.long).to(device)

# å®ä¾‹åŒ–æ¨¡å‹
model = SimpleMLP(input_dim, hidden_dim, output_dim).to(device)

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

print("--- åˆå§‹çŠ¶æ€æ£€æŸ¥ ---")
print(f"X (è¾“å…¥æ•°æ®): shape={X.shape}, requires_grad={X.requires_grad}, grad_fn={X.grad_fn}")
print(f"y_true (æ ‡ç­¾): shape={y_true.shape}, requires_grad={y_true.requires_grad}, grad_fn={y_true.grad_fn}\n")

# æ£€æŸ¥æ¨¡å‹çš„å‚æ•° W1, b1, W2, b2
# model.named_parameters() è¿”å›å‚æ•°åç§°å’Œå‚æ•°å¼ é‡æœ¬èº«
print("æ¨¡å‹å‚æ•°åˆå§‹çŠ¶æ€:")
for name, param in model.named_parameters():
    print(f"  {name}: shape={param.shape}, requires_grad={param.requires_grad}, grad_fn={param.grad_fn}, grad={param.grad}")
    # æ³¨æ„ï¼šæ‰€æœ‰å‚æ•°çš„ requires_grad éƒ½æ˜¯ Trueï¼Œå› ä¸ºå®ƒä»¬éœ€è¦è¢«ä¼˜åŒ–
    # grad_fn éƒ½æ˜¯ Noneï¼Œå› ä¸ºå®ƒä»¬æ˜¯â€œå¶å­èŠ‚ç‚¹â€ï¼Œæ˜¯ç›´æ¥åˆ›å»ºçš„ï¼Œä¸æ˜¯é€šè¿‡è¿ç®—å¾—æ¥ã€‚
    # grad éƒ½æ˜¯ Noneï¼Œå› ä¸ºè¿˜æ²¡æœ‰è¿›è¡Œåå‘ä¼ æ’­ã€‚
print("-" * 50)


# --- 2. å‰å‘ä¼ æ’­ï¼šæ„å»ºè®¡ç®—å›¾ ---
print("\n--- å‰å‘ä¼ æ’­å¼€å§‹ï¼šä¸€æ­¥æ­¥çœ‹ grad_fn çš„å˜åŒ– ---")

# æ­¥éª¤ 2.0: æ¸…é›¶æ¢¯åº¦ (ä¹ æƒ¯ä¸Šï¼Œè™½ç„¶è¿™é‡Œæ˜¯ç¬¬ä¸€æ¬¡ï¼Œä½†æ¨¡å‹å‚æ•°å¯èƒ½å·²ç»æœ‰å†å²æ¢¯åº¦)
optimizer.zero_grad()
print("æ‰€æœ‰æ¨¡å‹å‚æ•°çš„ .grad å·²æ¸…é›¶ã€‚")

# æ­¥éª¤ 2.1: é€šè¿‡ fc1 å±‚
# x æ˜¯ (5, 4)ï¼Œ self.fc1.weight æ˜¯ (hidden_dim, input_dim) å³ (5, 4)
# çŸ©é˜µä¹˜æ³• X @ W_transpose + b
# (5, 4) @ (4, 5) -> (5, 5) + (5,) -> (5, 5)
fc1_output = model.fc1(X)
print(f"\nfc1_output (X @ W1.T + b1): shape={fc1_output.shape}")
print(f"  requires_grad={fc1_output.requires_grad}") # True
# æ­¤æ—¶ fc1_output çš„ grad_fn ä¼šæ˜¯ä¸€ä¸ªä»£è¡¨çº¿æ€§å±‚æ“ä½œçš„ Function
# (å¦‚ AddmmBackward0ï¼Œå› ä¸ºå®ƒåŒ…å«äº†çŸ©é˜µä¹˜æ³•å’ŒåŠ æ³•)
print(f"  grad_fn={fc1_output.grad_fn}")
# fc1_output å†…éƒ¨çš„ grad_fn ä¼šè®°ä½ï¼š
#   - å®ƒæ˜¯ç”± model.fc1 è¿™ä¸ª nn.Linear æ¨¡å—äº§ç”Ÿçš„ã€‚
#   - å®ƒéœ€è¦ model.fc1.weight å’Œ model.fc1.bias æ¥è®¡ç®—æ¢¯åº¦ã€‚
#   - å®ƒä¼šå­˜å‚¨ Xï¼ˆè¾“å…¥ï¼‰çš„å¼•ç”¨ï¼Œä»¥ä¾¿è®¡ç®— X çš„æ¢¯åº¦ï¼ˆå¦‚æœ X å…è®¸æ±‚å¯¼ï¼‰ã€‚


# æ­¥éª¤ 2.2: é€šè¿‡ ReLU æ¿€æ´»å‡½æ•°
relu_output = model.relu(fc1_output)
print(f"\nrelu_output (ReLU(fc1_output)): shape={relu_output.shape}")
print(f"  requires_grad={relu_output.requires_grad}") # True
# relu_output çš„ grad_fn ä¼šæ˜¯ä¸€ä¸ªä»£è¡¨ ReLU æ“ä½œçš„ Function
print(f"  grad_fn={relu_output.grad_fn}") # ä¾‹å¦‚ <ReLUBackward0 object at ...>
# relu_output å†…éƒ¨çš„ grad_fn ä¼šè®°ä½ï¼š
#   - å®ƒçš„è¾“å…¥æ˜¯ fc1_outputã€‚
#   - å®ƒä¼šå­˜å‚¨ fc1_output çš„å¼•ç”¨ä»¥åŠåœ¨ ReLU æ“ä½œä¸­å“ªäº›å€¼æ˜¯æ­£çš„ï¼ˆç”¨äºæ¢¯åº¦è®¡ç®—ï¼‰ã€‚


# æ­¥éª¤ 2.3: é€šè¿‡ fc2 å±‚ (æœ€ç»ˆè¾“å‡º logits)
# relu_output æ˜¯ (5, 5)ï¼Œ self.fc2.weight æ˜¯ (output_dim, hidden_dim) å³ (10, 5)
# (5, 5) @ (5, 10) -> (5, 10) + (10,) -> (5, 10)
logits = model.fc2(relu_output)
print(f"\nlogits (æœ€ç»ˆè¾“å‡º): shape={logits.shape}")
print(f"  requires_grad={logits.requires_grad}") # True
# logits çš„ grad_fn ä¼šæ˜¯ä¸€ä¸ªä»£è¡¨çº¿æ€§å±‚æ“ä½œçš„ Function
print(f"  grad_fn={logits.grad_fn}") # ä¾‹å¦‚ <AddmmBackward0 object at ...>
# logits å†…éƒ¨çš„ grad_fn ä¼šè®°ä½ï¼š
#   - å®ƒæ˜¯é€šè¿‡ model.fc2 äº§ç”Ÿçš„ã€‚
#   - å®ƒéœ€è¦ model.fc2.weight å’Œ model.fc2.bias æ¥è®¡ç®—æ¢¯åº¦ã€‚
#   - å®ƒä¼šå­˜å‚¨ relu_output çš„å¼•ç”¨ã€‚


# æ­¥éª¤ 2.4: è®¡ç®—æŸå¤± (äº¤å‰ç†µ)
# criterion(logits, y_true)
# CrossEntropyLoss å†…éƒ¨ä¼šå…ˆå¯¹ logits è¿›è¡Œ LogSoftmaxï¼Œå†è¿›è¡Œ NLLLoss
loss = criterion(logits, y_true)
print(f"\nLoss (äº¤å‰ç†µæŸå¤±): shape={loss.shape}")
print(f"  requires_grad={loss.requires_grad}") # True
# æŸå¤±å¼ é‡é€šå¸¸æ˜¯æ ‡é‡ (shape=torch.Size([])), å®ƒçš„ grad_fn ä¼šæŒ‡å‘æŸå¤±å‡½æ•°çš„åå‘è®¡ç®—
print(f"  grad_fn={loss.grad_fn}") # ä¾‹å¦‚ <NllLossBackward0 object at ...>
# loss å†…éƒ¨çš„ grad_fn ä¼šè®°ä½ï¼š
#   - å®ƒçš„è¾“å…¥æ˜¯ logits å’Œ y_trueã€‚
#   - å®ƒéœ€è¦ logits çš„å€¼å’Œ y_true çš„å€¼æ¥è®¡ç®—æ¢¯åº¦ã€‚

print("\næ­¤æ—¶ï¼Œä¸€ä¸ªä» loss å›æº¯åˆ° W1, b1, W2, b2 çš„è®¡ç®—å›¾å·²åŠ¨æ€æ„å»ºå®Œæˆã€‚")
print("-" * 50)


# --- 3. åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦ ---
print("\n--- åå‘ä¼ æ’­å¼€å§‹ï¼šä» Loss å›æº¯è®¡ç®—æ¢¯åº¦ ---")

# è°ƒç”¨ backward() æ–¹æ³•
loss.backward()

print("\nåå‘ä¼ æ’­å®Œæˆï¼æ£€æŸ¥å‚æ•°çš„ .grad å±æ€§ã€‚")
# æ£€æŸ¥æ¨¡å‹å‚æ•°çš„æ¢¯åº¦
for name, param in model.named_parameters():
    print(f"  {name}:")
    print(f"    - grad_fn: {param.grad_fn}") # ä»æ˜¯ Noneï¼Œå› ä¸ºå®ƒä»¬æ˜¯å¶å­èŠ‚ç‚¹
    print(f"    - grad is None: {param.grad is None}")
    if param.grad is not None:
        print(f"    - grad.shape: {param.grad.shape}")
        # æ¢¯åº¦å€¼ï¼šæ˜¯ loss å¯¹è¯¥å‚æ•°ä¸­æ¯ä¸ªå…ƒç´ çš„åå¯¼æ•°
        # å®ƒä»¬æ˜¯å’Œå‚æ•°å½¢çŠ¶ç›¸åŒçš„ Tensor
        # print(f"    - grad value: \n{param.grad}") # å€¼æ¯”è¾ƒå¤§ï¼Œä¸æ‰“å°äº†ï¼Œä½†ä½ å¯ä»¥è‡ªè¡Œæ‰“å°
print("-" * 50)


# --- 4. å‚æ•°æ›´æ–°ï¼šä¼˜åŒ–å™¨æ ¹æ®æ¢¯åº¦æ›´æ–°å‚æ•° ---
print("\n--- å‚æ•°æ›´æ–°é˜¶æ®µï¼šä¼˜åŒ–å™¨åˆ©ç”¨ .grad æ›´æ–°å‚æ•° ---")

# è·å–å‚æ•°æ›´æ–°å‰çš„å€¼ (åªå–ä¸€éƒ¨åˆ†ä¸ºä¾‹)
old_fc1_weight_val = model.fc1.weight.data[0, 0].item()
old_fc2_bias_val = model.fc2.bias.data[0].item()
print(f"æ›´æ–°å‰ model.fc1.weight[0,0]: {old_fc1_weight_val:.4f}")
print(f"æ›´æ–°å‰ model.fc2.bias[0]: {old_fc2_bias_val:.4f}")

optimizer.step() # ä¼˜åŒ–å™¨æ‰§è¡Œä¸€æ­¥æ›´æ–°

print("\nä¼˜åŒ–å™¨æ›´æ–°å®Œæˆï¼å‚æ•°å€¼å·²æ”¹å˜ï¼Œ.grad å·²æ¸…é›¶ã€‚")
# æ£€æŸ¥å‚æ•°æ›´æ–°åçš„å€¼å’Œæ¢¯åº¦çŠ¶æ€
for name, param in model.named_parameters():
    print(f"  {name}:")
    # æ¢¯åº¦å·²ç»è¢«æ¸…é›¶äº† (é€šå¸¸æ˜¯ Noneï¼Œæˆ–è€…ä¸€ä¸ªå…¨é›¶å¼ é‡ï¼Œå–å†³äºä¼˜åŒ–å™¨å®ç°)
    print(f"    - grad is None: {param.grad is None}")

# è·å–å‚æ•°æ›´æ–°åçš„å€¼
new_fc1_weight_val = model.fc1.weight.data[0, 0].item()
new_fc2_bias_val = model.fc2.bias.data[0].item()
print(f"æ›´æ–°å model.fc1.weight[0,0]: {new_fc1_weight_val:.4f}")
print(f"æ›´æ–°å model.fc2.bias[0]: {new_fc2_bias_val:.4f}")
print(f"fc1.weight[0,0] å˜åŒ–é‡: {new_fc1_weight_val - old_fc1_weight_val:.4f}")
print(f"fc2.bias[0] å˜åŒ–é‡: {new_fc2_bias_val - old_fc2_bias_val:.4f}")

# å¯ä»¥é‡å¤æ•´ä¸ªå¾ªç¯ï¼ˆå‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€å‚æ•°æ›´æ–°ï¼‰æ¥è¿›è¡Œå¤šè½®è®­ç»ƒ
print("\n--- ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒæ­¥éª¤å®Œæˆ ---")
```

------

### è¯¦å°½è¯´æ˜ä¸æ€»ç»“ï¼š

1. **`X` å’Œ `y_true` (è¾“å…¥å’Œæ ‡ç­¾)ï¼š** å®ƒä»¬çš„ `requires_grad=False` (é€šå¸¸)ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä¸å¸Œæœ›è®¡ç®—æŸå¤±å¯¹å®ƒä»¬æœ¬èº«çš„æ¢¯åº¦ã€‚å®ƒä»¬çš„ `grad_fn` ä¹Ÿæ˜¯ `None`ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åŸå§‹æ•°æ®ï¼Œä¸æ˜¯é€šè¿‡å…¶ä»–å¼ é‡è¿ç®—å¾—æ¥çš„ã€‚

2. `model.parameters()` ä¸­çš„ `param` (W1, b1, W2, b2)ï¼š

   - å®ƒä»¬çš„ `requires_grad=True`ï¼šPyTorch çŸ¥é“éœ€è¦ä¸ºè¿™äº›å‚æ•°è®¡ç®—æ¢¯åº¦ã€‚
   - å®ƒä»¬çš„ `grad_fn` åˆå§‹æ—¶ä¸º `None`ï¼šå› ä¸ºå®ƒä»¬æ˜¯æ¨¡å‹å®šä¹‰æ—¶ç›´æ¥åˆ›å»ºçš„ï¼Œæ˜¯è®¡ç®—å›¾çš„**å¶å­èŠ‚ç‚¹**ã€‚
   - å®ƒä»¬çš„ `grad` åˆå§‹æ—¶ä¸º `None`ï¼šå› ä¸ºå°šæœªè¿›è¡Œåå‘ä¼ æ’­ã€‚

3. å‰å‘ä¼ æ’­ä¸­çš„ä¸­é—´ `Tensor` ( `fc1_output`, `relu_output`, `logits`, `loss` )ï¼š

   - å®ƒä»¬çš„ `requires_grad=True`ï¼šå› ä¸ºå®ƒä»¬æ˜¯ç”±å¸¦æœ‰ `requires_grad=True` çš„å‚æ•°ï¼ˆä¾‹å¦‚ `W1`, `b1`ï¼‰å‚ä¸è¿ç®—å¾—åˆ°çš„ã€‚

   - å®ƒä»¬çš„ `grad_fn` **ä¸ä¸º `None`**ï¼šæ¯ä¸€ä¸ª `grad_fn` éƒ½æ˜¯ä¸€ä¸ªç‰¹å®šçš„ `Function` å¯¹è±¡ï¼ˆå¦‚ `AddmmBackward0`, `ReLUBackward0`, `NllLossBackward0`ï¼‰ã€‚

   - `grad_fn` çš„ä½œç”¨å’Œå¦‚ä½•è®°å½•è®¡ç®—å›¾ï¼š

     - å½“

        

       ```
       fc1_output = model.fc1(X)
       ```

        

       å‘ç”Ÿæ—¶ï¼Œä¸€ä¸ª

        

       ```
       AddmmBackward0
       ```

        

       å¯¹è±¡è¢«åˆ›å»ºï¼Œå¹¶èµ‹å€¼ç»™

        

       ```
       fc1_output.grad_fn
       ```

       ã€‚è¿™ä¸ª

        

       ```
       AddmmBackward0
       ```

        

       å†…éƒ¨ä¼šå­˜å‚¨

       ï¼š

       - **å¯¹ `X` çš„å¼•ç”¨** (å¦‚æœ `X.requires_grad=True`ï¼Œè¿˜ä¼šä¿ç•™ `X` ä»¥ä¾¿è®¡ç®— `X` çš„æ¢¯åº¦)ã€‚
       - **å¯¹ `model.fc1.weight` å’Œ `model.fc1.bias` çš„å¼•ç”¨**ã€‚
       - **ç”¨äºæ‰§è¡Œåå‘ä¼ æ’­çš„è®¡ç®—é€»è¾‘**ï¼ˆçŸ¥é“å¦‚ä½•è®¡ç®— `d(loss)/d(fc1_output)` ä¹‹åï¼Œå¦‚ä½•å°†å…¶åˆ†è§£æˆ `d(loss)/dX`ã€`d(loss)/dW1` å’Œ `d(loss)/db1`ï¼‰ã€‚

     - å½“

        

       ```
       relu_output = model.relu(fc1_output)
       ```

        

       å‘ç”Ÿæ—¶ï¼Œä¸€ä¸ª

        

       ```
       ReLUBackward0
       ```

        

       å¯¹è±¡è¢«åˆ›å»ºï¼Œå¹¶èµ‹å€¼ç»™

        

       ```
       relu_output.grad_fn
       ```

       ã€‚è¿™ä¸ª

        

       ```
       ReLUBackward0
       ```

       

       å†…éƒ¨ä¼šå­˜å‚¨

       ï¼š

       - **å¯¹ `fc1_output` çš„å¼•ç”¨**ã€‚
       - **å‰å‘ä¼ æ’­æ—¶ `fc1_output` çš„å€¼**ï¼ˆå› ä¸ºå®ƒéœ€è¦çŸ¥é“å“ªäº›å…ƒç´ æ˜¯æ­£çš„ï¼Œå“ªäº›æ˜¯è´Ÿçš„ï¼Œæ‰èƒ½è®¡ç®— ReLU çš„æ¢¯åº¦ï¼‰ã€‚

     - `logits` å’Œ `loss` çš„ `grad_fn` ä¹Ÿæ˜¯ç±»ä¼¼çš„æ–¹å¼ï¼Œæ•è·æ‰€æœ‰å¿…è¦çš„ä¿¡æ¯å’Œå¯¹å…¶è¾“å…¥ `Tensor` çš„å¼•ç”¨ã€‚

   - **ä¸­é—´ `Tensor` çš„ `grad` å±æ€§ï¼š** é»˜è®¤æƒ…å†µä¸‹ï¼ŒPyTorch **ä¸ä¼š**ä¸ºä¸­é—´ `Tensor` å­˜å‚¨ `.grad` å±æ€§ï¼ˆå³å®ƒä»¬çš„ `.grad` ä¿æŒ `None`ï¼‰ã€‚è¿™æ˜¯ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œå› ä¸ºæˆ‘ä»¬é€šå¸¸åªå…³å¿ƒå¶å­èŠ‚ç‚¹ï¼ˆæ¨¡å‹å‚æ•°ï¼‰çš„æ¢¯åº¦ã€‚å¦‚æœä½ ç¡®å®éœ€è¦ä¸­é—´ `Tensor` çš„æ¢¯åº¦ï¼Œä½ éœ€è¦ä½¿ç”¨ `tensor.retain_grad()` æ–¹æ³•ã€‚

4. `loss.backward()` çš„æœºåˆ¶ï¼š

   - å®ƒä» `loss` å¼€å§‹ï¼Œæ²¿ç€ `loss.grad_fn` æŒ‡å‘çš„ `Function` å¯¹è±¡ã€‚
   - è¿™ä¸ª `Function` å¯¹è±¡ï¼ˆä¾‹å¦‚ `NllLossBackward0`ï¼‰ä¼šæ‰§è¡Œå…¶å†…éƒ¨çš„åå‘è®¡ç®—æ–¹æ³•ï¼ˆé€šå¸¸æ˜¯ `backward()` æ–¹æ³•çš„ C++ å®ç°ï¼‰ã€‚å®ƒä¼šå–å‡ºä¹‹å‰æ•è·çš„ `logits` å’Œ `y_true` ç­‰ä¿¡æ¯ï¼Œè®¡ç®— `loss` å¯¹ `logits` çš„æ¢¯åº¦ï¼Œå¹¶å°†è¿™ä¸ªæ¢¯åº¦ä¼ é€’ç»™ `logits`ã€‚
   - ç„¶åï¼Œ`logits.grad_fn` è¢«æ¿€æ´»ï¼Œè®¡ç®— `logits` å¯¹ `relu_output`ã€`W2`ã€`b2` çš„æ¢¯åº¦ã€‚`d(loss)/d(W2)` å’Œ `d(loss)/d(b2)` çš„éƒ¨åˆ†ç´¯åŠ åˆ° `W2.grad` å’Œ `b2.grad` ä¸­ã€‚
   - ä¾æ­¤ç±»æ¨ï¼Œè¿™ä¸ªæ¢¯åº¦æµä¼šä¸€ç›´åå‘ä¼ æ’­ï¼Œç›´åˆ° `W1` å’Œ `b1`ã€‚
   - æ¯ä¸ª `grad_fn` éƒ½åƒä¸€ä¸ªâ€œé€†å‘æ“ä½œâ€çš„å°è£…å™¨ï¼ŒçŸ¥é“å¦‚ä½•æ ¹æ®è¾“å‡ºæ¢¯åº¦å’Œå‰å‘ä¼ æ’­æ—¶ä¿å­˜çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè®¡ç®—å…¶è¾“å…¥çš„æ¢¯åº¦ã€‚

5. `optimizer.step()` å’Œæ¢¯åº¦æ¸…é›¶ï¼š

   - `optimizer.step()` ä¼šéå† `model.parameters()` è¿”å›çš„æ‰€æœ‰å¶å­èŠ‚ç‚¹å‚æ•°ã€‚
   - å®ƒä»æ¯ä¸ªå‚æ•°çš„ `.grad` å±æ€§ä¸­è·å–æ¢¯åº¦å€¼ã€‚
   - æ ¹æ®ä¼˜åŒ–å™¨è‡ªèº«çš„ç®—æ³•ï¼ˆä¾‹å¦‚ SGD çš„ `param = param - learning_rate * param.grad`ï¼‰ï¼Œæ›´æ–°å‚æ•°çš„å€¼ã€‚
   - **é‡è¦çš„ï¼š** `optimizer.step()` ä¹‹åï¼Œé€šå¸¸ä¼šè°ƒç”¨ `optimizer.zero_grad()` æˆ–è€…ä¼˜åŒ–å™¨å†…éƒ¨ä¼šè‡ªåŠ¨æ¸…é›¶æ¢¯åº¦ï¼ˆå–å†³äºå®ç°ï¼Œä½† PyTorch æ¨èæ‰‹åŠ¨ `zero_grad()`ï¼‰ã€‚è¿™æ˜¯å› ä¸º PyTorch é»˜è®¤ä¼š**ç´¯åŠ æ¢¯åº¦**ã€‚å¦‚æœä¸æ¸…é›¶ï¼Œä¸‹æ¬¡ `backward()` æ—¶ï¼Œæ–°çš„æ¢¯åº¦ä¼šåŠ åˆ°æ—§çš„æ¢¯åº¦ä¸Šï¼Œå¯¼è‡´é”™è¯¯çš„æ›´æ–°ã€‚

ç°åœ¨ï¼Œä½ æ˜¯ä¸æ˜¯è§‰å¾—è¿™äº›â€œé­”æ³•â€å˜å¾—æ›´å…·ä½“å’Œå¯ç†è§£äº†å‘¢ï¼Ÿè¿™ç§åŠ¨æ€è®¡ç®—å›¾çš„æ„å»ºå’Œå›æº¯ï¼Œæ˜¯ PyTorch å¼ºå¤§è€Œçµæ´»çš„å…³é”®ã€‚