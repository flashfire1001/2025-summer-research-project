1. 白天大部分时间在跟PHD XJM battle关于$$u_t^{\text{target}}$$的构造，并且搞清楚了。（可能不对，这个只是我个人的理解）

   

2. 了解Preceptron, activation function, 简单的MLP，(自己只简单算了一个XOR？多于两层的没自己试过)，还有Feedforward networks

   但是，还是不理解一个MLP为什么可以通过非线性的representation之后（隐藏层），通过一些activation function就能达到函数逼近的效果。感性的讲，似乎如果我们接受“傅立叶/泰勒”的思想，就是用$$\sigma$$ 函数来拟合？我们似乎通过调整w，让这个$$\sigma$$像一个“跳跃函数”（忘记叫什么名字了那个函数），然后通过很多个这种叠加，我们就可以“逼近任何弯弯绕绕的函数”。这个好不严谨，没太看明白Universal Approximation Theorem。明天研究一下。

   

3. 本来晚上不学了。跑完步洗完澡看见了嘉铭哥的learning log，倍感羞愧，又看了一下怎么算的梯度下降来train，但是还没看明白太困了睡了

明天打算继续看NN，先搞懂这个Universal Approximation Theorem的数学原理和train NN的原理，然后试着完成一下MIT对应Lab的第一个ODE/SDE。