2025-06-17

1. 昨日的continuity function收尾+vector calculus整理，收尾Note P18，完成3.2的学习。

   出于虚荣和形式主义作风，让gpt做的面子工程: [vector calculus](https://hanbinzheng.github.io/posts/Gradient,-Directional-Derivative,-and-Tangent-Geometry/), [continuity equation](https://hanbinzheng.github.io/posts/The-Continuity-Equation/)

   

2. 未完成pytorch: DataSets & Data Loader 

   记不住代码什么意思，但是似乎能看明白他在干嘛？

   

3. 复习lecture note P9-12，重新理解“生成模型”的“生成”的含义（从新分布中采样），以及这个conceptual definition和实际操作（采样高斯，然后用vector field transform）的区别。我们并没有直接操作distribution，我们操作的是velocity vector field。只是conceptually，等价于transform RV/distribution

   

4. 试图看懂Fokker-Planck Equation，经过尝试发现似乎是diffusion的内容，遂放弃，跳过MIT Note的P19-23 （先做出来再说）

   

5. 初步理解loss function设计上为什么使用conditional vector field和marginal vector field是等价的。（MIT Note P24-27）但是对于一个积分交换的仍有疑惑。但是不妨假设函数性质足够好，那么就可以交换积分顺序了。考虑到暑研目的是搞清楚flow matching model，不是学数学，因此放弃证明。



明天任务，了解一下NN的基本原理（试着上手试一试？），重新回顾一遍整个flow matching model的原理（能从头到尾重现一遍推导）









关于神经网络

---

gpt给出的适合这个flow matching的NN学习规划

1. **理论层面**

   

   - 理解什么是神经网络（NN）——函数逼近器，本质上是复合函数。
   - 掌握最基本的结构：多层感知机（MLP / feedforward NN）。
   - 理解前向传播（forward pass）和反向传播（backpropagation）机制。
   - 理解训练目标（loss function）与梯度下降优化的关系。
   - 理解激活函数的意义（非线性）、层的作用，以及为什么要用深度结构。
   - 懂得参数是怎么通过数据训练出来的（优化问题）。

   

2. **实际层面**

   

   - 能看懂 PyTorch 中的 nn.Module 类。
   - 明白如何使用 torch.nn.Linear 构建全连接层。
   - 懂得如何定义一个 NN、前向传播函数、loss function。
   - 明白 optimizer.step() 在做什么。
   - 懂得如何搭建、训练和测试一个 NN。

   

3. **flow matching 相关**

   

   - 明白 NN 在 flow model 里是如何代表时间相关向量场 u^\theta_t(x) 的。
   - 懂得 NN 输入 (x, t) 输出 vector u^\theta_t(x) 的含义。
   - 理解训练目标与 loss function 的数学结构。
   - 会写训练 loop，哪怕只用 PyTorch 最简单的 API。

如果你接受这个标准，我将给你一套分阶段结构化的讲解，包括：

1. **神经网络基本原理**（从感知机讲起）
2. **梯度下降与训练机制**
3. **PyTorch 中的 NN 构建方式**
4. **在 flow matching 中的 NN 用法解析**