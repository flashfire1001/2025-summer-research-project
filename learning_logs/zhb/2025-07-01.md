今天重新写了一遍生成mnist的文件，删掉了一些不必要的部分，对class进行了一些个人认为更合理的重命名。本来意图是写一个兼容mnist和cifar的unet版本的，但是没有写完。

但是写的过程中对于一些操作有了更深层次的理解，比如利用tensor作为index赋值，还有傅立叶编码的时候为什么要乘$$/sqrt{2}$$，还有ResidualLayer(个人认为应该叫AdaptLayer)里，卷积和加上residual的顺序。还有一些，比如先conv，再batchnorm，最后激活和先激活，再batchnorm，最后conv在意图上的区别。

本来打算写完生成cifar10的，但是因为有同学来玩，加上早上洗完衣服收拾完房间之后累了不想干活，没写完。明天补上trainer和sampleable的部分试试看能不能跑。

感觉今天学的很杂，似乎什么都没有学到，但是又似乎学到了什么。不管怎么说，代码熟练了很多。
