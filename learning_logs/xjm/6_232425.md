完成lab1 和lab2 .完成了2个比较简单的classification项目熟悉代码.

lab3许多地方看不懂. 进度根本推进不下去.

关于diffusion mode和score matchingl :

diffusion model 的关键转化步骤: langevin dynamics
$$
dX_t = \left[u_t^{\theta}(x) + \frac{1}{2}\sigma^2 s_t^{\theta}(x)\right] dt + \sigma \sqrt{dt} \epsilon
$$
关于MIT第五章:

classifer Guidance 的理论中,我理解了在lab3代码中为什么label_y屡屡缺席.我开始有点疑惑,感觉样例代码没有按照y 的条件去取样.后来明白了只需要y同时绑定这图片一起取出就行了. 图片库本身就是y和z和联合分布, y和不正确的z也是可能发生的(只不过,本身是一张'3'的图片却标着4的label,这种情况我们要尽量减小它的概率). 我之前觉得y和z应该是同一个东西,毕竟你用手写下一个数字自动就有了一个digit-label与之对应;这种想法是错误的.

通过看一些网上的文章和问题AI,得知,原来是这样:

1.   MNIST比较傻的方案:Per-class Generator **“one model per class” 的 naive 多模型生成法**,无classifier.直接hardcode上去就是了.
2.   MNIST比较基础的方案:先训练一个classifier,(分类问题用CNN), 再训练一个速度场满足:

​	v_guided(x, t) = v(x, t) + guidance_weight * ∇ₓ log p(y|x) — 这能让速度场被分类器的输出p(y|x) 引导.也是非常值得做为练手的部分.

3.   MNIST比较聪明的方案:同时训练生成模型的两个mode：一个是 **有条件的（知道 y）**，一个是 **无条件的（不知道 y,给一个default值embedding填充进去**) 在训练的时候按照一定比例,给模型输入一个图像 x，以及对应的标签 y（比如数字 3）但另一部分时候，**把标签 y 故意设为 None**（或一个“空标签”.)混合两个mode引导输出:guided_output = (1 + w) * model(x, y) - w * model(x, None)

很模糊的地方终于搞清晰了!我们选方案3

 Things I need (plan) to learn:
- Basic computer vision and torchvision, For Image data Augmentation
- the math fundamentals for training in terms of score matching and CFG(classifier - free)
- some basic some insight in tuning the hyperparameter and choosing the architecture
